#!/usr/bin/env python
# Backs up installers, extras, etc. for all your GOG.com purchases.
#
# Copyright (C) 2011, 2012  Evan Powers
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Usage: %prog [options] <command> [<args>]

Commands:

  login <email> <password>
    Login to GOG.com using the supplied email and password.

  manifest
    Create a manifest describing all games and extras owned by the
    current account and including all available verification metadata.

  list
    Print a summary of the current manifest.

  compare
  update
    Verify downloaded files by comparing them against the manifest,
    and print a report on what must be downloaded. 'compare' validates
    everything; 'update' only validates new files.

  fetch
    Download any missing, incomplete, or corrupted files (after an
    implicit 'update').
"""

__author__ = 'Evan Powers'
__version__ = '1.0'
__url__ = 'https://github.com/evanpowers/gog-backup'

import sys, os, optparse, contextlib, pprint, hashlib, zipfile
import cookielib, urllib, urllib2, urlparse
import xml.etree.ElementTree
import threading, Queue, time
import StringIO, traceback

import html5lib  # http://code.google.com/p/html5lib/

BACKUPINTO = '.'
CONCURRENCY = 6
BREADTHFIRST = False
FETCHCOVERS = False
PATHMAP = '.gog.pathmap.txt'
COOKIES = '.gog.cookies'
MANIFEST = '.gog.games.py'
VALIDATED = '.gog.valid.py'

LOGIN = 'https://www.gog.com/en/login'
SHELF = 'https://www.gog.com/en/myaccount/shelf'
LIST = 'https://www.gog.com/en/myaccount/list'
THUMB = 'http://www.gog.com'

pathmap = {}
cookiejar = cookielib.LWPCookieJar(COOKIES)
cookieproc = urllib2.HTTPCookieProcessor(cookiejar)
opener = urllib2.build_opener(cookieproc)
treebuilder = html5lib.treebuilders.getTreeBuilder('etree')
parser = html5lib.HTMLParser(tree=treebuilder, namespaceHTMLElements=False)
debuglog = None

useragent = 'gog-backup/%s (%s)' % (__version__, __url__)
opener.addheaders = [('User-agent', useragent)]

class DebugPage(Exception):
    pass

class AttrDict(dict):
    def __init__(self, **kw):
        self.update(kw)
    def __getattr__(self, key):
        return self[key]
    def __setattr__(self, key, val):
        self[key] = val

class Page(AttrDict):
    def read(self, size):
        if not hasattr(self.page, 'read'):
            raise DebugPage(self.url)
        return self.page.read(size)
    def close(self):
        if hasattr(self.page, 'close'):
            self.page.close()

def debug_load(fn):
    log, _ = debuglog
    with zipfile.ZipFile(fn, 'r') as r:
        with r.open('log.py', 'rU') as log_:
            log_ = eval(log_.read())
        for dbk, pg in log_:
            if isinstance(pg, dict):
                pg = Page(**pg)
                if isinstance(pg.page, str):
                    pg.page = r.read(pg.page)
                assert dbk not in log
                log[dbk] = pg

def debug(code, *args, **kw):
    if not debuglog:
        return Page()
    log, fn = debuglog
    dbk = tuple([code] + sorted(kw.items()))
    if fn:
        if args:
            assert not kw
            pg = args
        else:
            pg = Page(**kw)
        log.append((dbk, pg))
        return pg
    else:
        if args:
            if code == 'TRACEBACK':
                import pdb
                pdb.post_mortem()
            return
        else:
            return log[dbk]

def debug_save():
    if not debuglog:
        return
    log, fn = debuglog
    if not fn:
        return
    with zipfile.ZipFile(fn, 'w') as w:
        for i, (dbk, pg) in enumerate(log):
            if isinstance(pg, Page):
                if isinstance(pg.page, str):
                    i = '%d.txt' % i
                    w.writestr(i, pg.page)
                    pg.page = i
                else:
                    pg.page = True
        w.writestr('log.py', pprint.pformat(log))

def _request(pg, url, args, range, hide):
    if not hide:
        print '>', url,
        sys.stdout.flush()
    if args is not None:
        args = urllib.urlencode(args)
    req = urllib2.Request(url, data=args)
    if range is not None:
        req.add_header('Range', 'bytes=%d-%d' % range)
    pg.page = opener.open(req)
    pg.url_before = req.get_full_url()
    pg.url_after = pg.page.geturl()
    pg.code = pg.page.getcode()
    if range is not None:
        pg.content_range = pg.page.headers['Content-Range']
    pg.headers = dict((k, v) for k, v in pg.page.headers.items()
                      if k != 'set-cookie')  # (only for debug)
    if not hide:
        print '[%d]' % pg.code
    assert 200 <= pg.code < 300
    if pg.url_after.endswith('/login') and url != LOGIN:
        # must have been redirected to a login page
        raise RuntimeError('login required')

def request(url, args=None, html=True, hide=False):
    pg = debug('REQUEST', url=url, args=args, html=html)
    if not hasattr(pg, 'page'):
        _request(pg, url, args, None, hide)
        with contextlib.closing(pg.page) as page:
            pg.page = page.read()
    if html:
        etree = parser.parse(pg.page)
    else:
        sio = StringIO.StringIO(pg.page)
        etree = xml.etree.ElementTree.parse(sio).getroot()
    return etree

def stream(url, args=None, range=None, hide=False):
    pg = debug('STREAM', url=url, args=args, range=range)
    if not hasattr(pg, 'page'):
        _request(pg, url, args, range, hide)
    return contextlib.closing(pg)

def load_attrdicts(fn):
    try:
        with open(fn, 'r') as r:
            ad = r.read().replace('{', 'AttrDict(**{').replace('}', '})')
        return eval(ad)
    except IOError:
        return AttrDict()

def locate(g, f):
    path = os.path.join(pathmap.get(g.key, g.key), f.name)
    if not os.path.isfile(path) and os.path.isfile(f.name):
        return f.name
    return path

def md5map(f, path):
    print '#', path
    H = hashlib.md5()
    part = []
    with open(path, 'rb') as r:
        for start, end, md5 in f.part:
            h = hashlib.md5()
            n = end + 1 - start
            r.seek(start)
            p = r.read(n)
            H.update(p)
            h.update(p)
            if len(p) < n:
                # assume the chunk is merely incomplete
                part.append((start + len(p), end))
            elif md5 != h.hexdigest():
                part.append((start, end))
    eq = f.md5 == H.hexdigest()
    if (eq and part) or (not eq and not part):
        # this actually happens with one of my games
        print 'WARNING: error in chunk XML?! GOG.com must have a bug somewhere.'
        print '    %s %s' % (f.md5, H.hexdigest())
        print '    %r' % (part,)
    return part

def zipmap(f, path):
    print '&', path
    part = []
    with zipfile.ZipFile(path, 'r') as r:
        if r.testzip() is not None:
            part = [(0, f.size-1)]
    return part

def compare(valid):
    # compare what we have to the manifest
    games = load_attrdicts(MANIFEST)
    missing, corrupt = [], []
    for g in games:
        for f in g.setup + g.extra:
            k = g.key, f.name
            u = (f.size, f.md5) if hasattr(f, 'md5') else f.size
            v = valid.get(k)
            if not v or v.uniq != u:
                v = AttrDict(key=k, uniq=u, need=[(0, f.size-1)])
                valid[k] = v
            elif not v.need:
                continue  # cache says we have this file

            # try to find a local copy
            path = locate(g, f)
            if not os.path.isfile(path):
                missing.append((g, f))
                continue

            # validate the file we have
            sz = os.path.getsize(path)
            if hasattr(f, 'part'):
                v.need = md5map(f, path)
            elif sz < f.size:
                # assume partial content isn't corrupted...
                v.need = [(sz, f.size-1)]
            elif sz > f.size:
                # ...but extra means total corruption
                v.need = [(0, f.size-1)]
                v.drop = True
            elif path.endswith('.zip'):
                # don't need metadata to validate a .zip
                v.need = zipmap(f, path)
            else:
                v.need = []
            if v.need:
                corrupt.append((g, f))

    # write the validity cache to disk
    with open(VALIDATED, 'w') as w:
        print >>w, '# %d files' % len(valid)
        pprint.pprint([valid[k] for k in sorted(valid)], width=100, stream=w)

    return valid, games, missing, corrupt

def megs(b):
    return '%.1fM' % (b / 1000.0**2)

def needed(valid, games, missing, corrupt):
    # print a simple report
    def need(g, f):
        v = valid[(g.key, f.name)]
        return sum(e + 1 - s for s, e in v.need)
    for txt, lst in (('~corrupt', corrupt), ('-missing', missing)):
        mb = megs(sum(need(g, f) for g, f in lst))
        print '%d files are %s (%s):' % (len(lst), txt[1:], mb)
        for g, f in lst:
            pcent = 100.0 * need(g, f) / f.size
            print txt[0], f.name,
            if pcent < 100:
                print '\t(%s, %d%%)' % (megs(need(g, f)), pcent),
            print

def open_notrunc(name, bufsize=4*1024):
    fd = os.open(name, os.O_WRONLY | os.O_CREAT, 0666)
    return os.fdopen(fd, 'wb', bufsize)

def cmd_login(email, passwd):
    etree = request(LOGIN, args={'log_email': email,
                                 'log_password': passwd})
    if etree.find(".//div[@id='register_holder']") is not None:
        # this element is only present if not logged in
        raise RuntimeError('login failed')
    cookiejar.save()

def cmd_manifest():
    games = {}

    # requests up front so we have them all in debug mode
    list_et = request(LIST)
    shelf_et = request(SHELF)

    # parse game list and available files for each from list page
    for game in list_et.findall(".//div[@class='tab_1_row']"):
        gamecard = game.find(".//div[@class='tab_1_title']/a")
        g = AttrDict()
        g.key = gamecard.attrib['href'].split('/')[-1]
        g.title = gamecard.text
        g.thumb = THUMB + game.find(".//img[@src]").attrib['src']
        g.setup, g.extra = [], []
        for row in game.findall(".//div[@class='sh_o_i_row']"):
            f = AttrDict()
            f.href = row.attrib.get('onclick', '')
            if not 'download/file' in f.href:
                f.href = row.find(".//a").attrib['href']
                g.setup.append(f)
            else:
                f.href = f.href[f.href.index("'")+1 : -1]
                f.desc = row.find(".//div[@class='sh_o_i_text']/span").text
                g.extra.append(f)
        games[g.key] = g

    # match games to covers on the shelf page
    for game in shelf_et.findall(".//div[@class='shelf_item_h']"):
        gamecard = game.find(".//div[@class='shelf_ov_tab_2_title']/a")
        if gamecard is not None:
            g = games[gamecard.attrib['href'].split('/')[-1]]
            g.cover = THUMB + game.find(".//img[@src]").attrib['src']

    # request a zero-length range from each file to determine
    # - the total size (from the Content-Range header)
    # - the target file name (from the post-redirect URL)
    # - the chunk MD5s for setup files (from the corresponding XML)
    for g in games.values():
        for i, f in enumerate(g.setup + g.extra):
            with stream(f.href, range=(0, 0)) as page:
                f.size = int(page.content_range.split('/')[-1])
                f.name = urlparse.urlparse(page.url_after).path.split('/')[-1]
                if i < len(g.setup):
                    f.url = page.url_after
        for f in g.setup:
            etree = request(f.url.replace('?', '.xml?'), html=False, hide=True)
            del f['url']
            assert f.name == etree.attrib['name']
            assert f.size == int(etree.attrib['total_size'])
            f.md5 = etree.attrib['md5']
            f.part = [(int(chunk.attrib['from']),
                       int(chunk.attrib['to']),
                       chunk.text)
                      for chunk in etree.findall(".//chunk[@method='md5']")]
            assert len(f.part) == int(etree.attrib['chunks'])
            f.part.sort()

    # write all this meta-data to disk
    with open(MANIFEST, 'w') as w:
        print >>w, '# %d games' % len(games)
        pprint.pprint(games.values(), width=100, stream=w)

    # optionally download cover and thumbnail
    if FETCHCOVERS:
        def fetch(which, g):
            href = getattr(g, which)
            path = pathmap.get(g.key, g.key)
            ext = os.path.splitext(href)[-1]
            fn = '%s_%s%s' % (g.key, which, ext)
            try:
                with stream(href) as page:
                    data = page.read()
                with open(os.path.join(path, fn), 'wb') as w:
                    w.write(data)
            except DebugPage:
                pass
        for g in games.values():
            fetch('cover', g)
            fetch('thumb', g)

def cmd_list():
    # summarize the manifest
    games = load_attrdicts(MANIFEST)
    valid = dict((v.key, v) for v in load_attrdicts(VALIDATED))
    total = 0
    for g in sorted(games, key=lambda g: g.key):
        ssz = sum(f.size for f in g.setup)
        esz = sum(f.size for f in g.extra)
        total += ssz + esz
        print '%s (%s)' % (g.key, g.title)
        print '    %8s in %d setup files' % (megs(ssz), len(g.setup))
        print '    %8s in %d extras' % (megs(esz), len(g.extra))
    print '%d games, a total of %s' % (len(games), megs(total))

def cmd_compare():
    # start the comparison from scratch
    needed(*compare({}))

def cmd_update():
    # compare only what's uncached
    valid = dict((v.key, v) for v in load_attrdicts(VALIDATED))
    needed(*compare(valid))

def cmd_fetch():
    # start an incremental comparison
    valid = dict((v.key, v) for v in load_attrdicts(VALIDATED))
    _, games, _, _ = compare(valid)
    sizes, rates, errors = {}, {}, {}

    # build a list of work items
    work = Queue.PriorityQueue()
    i = -sys.maxint
    for g in games:
        for f in g.setup + g.extra:
            v = valid[(g.key, f.name)]
            path = locate(g, f)
            dn = os.path.dirname(path)
            if dn and not os.path.isdir(dn):
                os.makedirs(dn)
            if os.path.isfile(path) and v.get('drop'):
                os.remove(path)
            i += 1
            for j, (start, end) in enumerate(v.need, -sys.maxint):
                # BREADTHFIRST: first chunk of all files, then second chunk...
                # else: all chunks of first file, then of second file...
                # ...after all chunks, extras in ascending size order
                if hasattr(f, 'part'):
                    prio = j if BREADTHFIRST else i
                else:
                    prio = end
                args = f, start, end, path
                work.put((prio, args))
                sz = end + 1 - start
                sizes[path] = sz + sizes.get(path, 0)

    # work item I/O loop
    def ioloop(tid, path, page, out):
        sz, t0 = True, time.time()
        while sz:
            buf = page.read(4*1024)
            t = time.time()
            out.write(buf)
            sz, dt, t0 = len(buf), t - t0, t
            with lock:
                sizes[path] -= sz
                rates.setdefault(path, []).append((tid, (sz, dt)))

    # worker thread main loop
    def worker():
        tid = threading.current_thread().ident
        while not work.empty():
            _, (f, start, end, path) = work.get()
            try:
                with open_notrunc(path, 4*1024) as out:
                    out.seek(start)
                    se = start, end
                    with stream(f.href, range=se, hide=True) as page:
                        hdr = page.content_range.split()[-1]
                        assert hdr == '%d-%d/%d' % (start, end, f.size)
                        assert out.tell() == start
                        ioloop(tid, path, page, out)
                        assert out.tell() == end + 1
            except IOError, e:
                with lock:
                    print >>sys.stderr, '!', path
                    errors.setdefault(path, []).append(e)
            except DebugPage:
                pass
            work.task_done()

    # detailed progress report
    def progress():
        with lock:
            left = sum(sizes.values())
            print
            for path, flowrates in sorted(rates.items()):
                flows = {}
                for tid, (sz, t) in flowrates:
                    szs, ts = flows.get(tid, (0, 0))
                    flows[tid] = sz + szs, t + ts
                bps = sum(szs/ts for szs, ts in flows.values())
                print '%10s %8.1fK/s %2dx  %s' % \
                    (megs(sizes[path]), bps / 1000.0, len(flows), path)
            print megs(left), 'remaining'
            rates.clear()

    # process work items with a thread pool
    lock = threading.Lock()
    pool = []
    for i in range(CONCURRENCY):
        t = threading.Thread(target=worker)
        t.daemon = True
        t.start()
        pool.append(t)
    try:
        while any(t.is_alive() for t in pool):
            progress()
            time.sleep(1)
    except:
        with lock:
            if errors:
                print >>sys.stderr, len(errors), 'files had errors:'
            for path in errors:
                print >>sys.stderr, '!', path
                for e in errors[path]:
                    print >>sys.stderr, '\t', e
        raise

def main(args):
    arg0 = sys.argv[0]

    class Parser(optparse.OptionParser):
        def format_epilog(self, formatter):
            return self.epilog + '\n'
    usage = __doc__.splitlines()
    parser = Parser(prog=arg0, usage=usage[0],
                    epilog='\n'.join(usage[1:]),
                    version='%prog version ' + __version__)
    parser.add_option('--debug', action='store', metavar='ZIPFILE',
                      help='generate a HTTP request log')
    parser.add_option('--replay', action='store', metavar='ZIPFILE',
                      help='replay an existing HTTP request log')
    opts, args = parser.parse_args()
    if opts.debug and opts.replay:
        parser.error('options --debug and --replay are mutually exclusive')

    # configure debugging mode
    global debuglog
    if opts.debug:
        debuglog = ([], opts.debug)
    if opts.replay:
        debuglog = ({}, None)
        debug_load(opts.replay)

    # switch to backup directory
    os.chdir(BACKUPINTO)

    # load saved cookies, if any
    try:
        cookiejar.load()
    except IOError:
        pass

    # load the path map, if any
    try:
        with open(PATHMAP, 'rU') as p:
            for line in p:
                k, v = line[:-1].split(None, 1)
                pathmap[k] = v
    except IOError:
        pass

    # dispatch to subcommand handler
    try:
        debug('INVOCATION', args[0])
    except IndexError:
        print >>sys.stderr, '%s: missing command' % arg0
        return 10
    try:
        fn = globals()['cmd_' + args[0]]
    except KeyError:
        print >>sys.stderr, '%s: invalid command %r' % (arg0, args[0])
        return 10
    try:
        fn(*args[1:])
    except TypeError, e:
        msg = e.message.replace('cmd_%s()' % args[0], repr(args[0]))
        print >>sys.stderr, '%s: %s' % (arg0, msg)
        return 10
    except RuntimeError, e:
        print >>sys.stderr, '%s: %s' % (arg0, e.message)
        return 1

    return 0

if __name__ == '__main__':
    try:
        sys.exit(main(sys.argv[1:]))
    except Exception:
        debug('TRACEBACK', traceback.format_exc())
        raise
    finally:
        debug_save()
